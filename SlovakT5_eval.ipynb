{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SlovakT5_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP86gINeqA0jlkSNI7IGK/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richardcepka/notebooks/blob/main/SlovakT5_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qqq\n",
        "!pip install sentencepiece -qqq\n",
        "!pip install datasets -qqq\n",
        "!pip install conllu -qqq\n",
        "!pip install flax -qqq\n",
        "!pip install ml-collections -qqq"
      ],
      "metadata": {
        "id": "fmGWoDsu2F9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric, Dataset, DatasetDict, concatenate_datasets\n",
        "from ml_collections import config_dict\n",
        "from transformers import T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer, Trainer"
      ],
      "metadata": {
        "id": "qR0Nrr93eCe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Task(ABC):\n",
        "    \n",
        "    @abstractmethod\n",
        "    def _prepare_data(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_data(self):\n",
        "        pass\n",
        "\n",
        "        \n",
        "    @abstractmethod\n",
        "    def map_string2id(self):\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def _map_to_seq2seq(self):\n",
        "        pass\n",
        "\n",
        "    \n",
        "    @abstractmethod\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "        pass"
      ],
      "metadata": {
        "id": "-sHcQbuTeAsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SST2(Task):\n",
        "    prefix = 'sst2'\n",
        "    prefix1 = 'veta'  # 'sentence' \n",
        "    id2string = {0:'negatívna', 1: 'pozitívna'}  # {0: 'negative', 1: 'positive'}  \n",
        "    string2id = {v: k for k, v in id2string.items()}\n",
        "    seed = 7\n",
        "\n",
        "    def __init__(self, seq2seq=True):\n",
        "        self._prepare_data()\n",
        "        if seq2seq:\n",
        "          self._map_to_seq2seq()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "      root_path = 'https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/'\n",
        "      dataset_names = [\n",
        "              'kinit_golden_accomodation.csv', \n",
        "              'kinit_golden_books.csv', \n",
        "              'kinit_golden_cars.csv', \n",
        "              'kinit_golden_games.csv', \n",
        "              'kinit_golden_mobiles.csv', \n",
        "              'kinit_golden_movies.csv', \n",
        "              'kinit_golden_stress.csv'\n",
        "      ]\n",
        "\n",
        "      self.dataset = load_dataset('csv', data_files=[root_path + name for name in dataset_names], header=None, names=['text', 'labels'], split='train')\n",
        "      self.dataset = self.dataset.filter(lambda example: example['labels'] != 0)  # drop labels == 0 (neutral)\n",
        "      self.dataset = self.dataset.map(lambda example: {'labels': max(example['labels'], 0)})  # -1 -> 0 (negative), 1 -> 1 (positive)\n",
        "\n",
        "      train_devtest = self.dataset.train_test_split(shuffle=True, seed=self.seed, test_size=0.4)\n",
        "      dev_test = train_devtest['test'].train_test_split(seed=self.seed, test_size=0.6)\n",
        "      self.dataset = DatasetDict(\n",
        "          {\n",
        "              'train': train_devtest['train'],\n",
        "              'validation': dev_test['train'],\n",
        "              'test': dev_test['test']\n",
        "          }\n",
        "      )\n",
        "    \n",
        "    def _map_to_seq2seq(self):\n",
        "      self.dataset = self.dataset.map(\n",
        "          lambda example: {\n",
        "              'text': self.prefix + ' ' + self.prefix1 + ': ' + example['text'], \n",
        "              'labels': self.id2string[example['labels']]\n",
        "          }\n",
        "      )\n",
        "\n",
        "    def map_string2id(self, string_label: str):\n",
        "        try:\n",
        "          return self.string2id[string_label]\n",
        "        except KeyError:\n",
        "          return -1\n",
        "\n",
        "    def get_data(self):\n",
        "      return self.dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "      return load_metric('glue', 'sst2')\n",
        "\n",
        "\n",
        "class STSB(Task):\n",
        "    prefix = 'stsb'\n",
        "    prefix1 = 'veta1'  # 'sentence1'  \n",
        "    prefix2 = 'veta2'  # 'sentence2' \n",
        "\n",
        "    def __init__(self, seq2seq=True):\n",
        "        self._prepare_data()\n",
        "        if seq2seq:\n",
        "          self._map_to_seq2seq()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "      self.dataset = load_dataset('crabz/stsb-sk')\n",
        "      self.dataset['train'] = Dataset.from_dict(self.dataset['train'][0])\n",
        "      self.dataset['validation'] = Dataset.from_dict(self.dataset['validation'][0])\n",
        "      self.dataset['test'] = Dataset.from_dict(self.dataset['test'][0])\n",
        "    \n",
        "    def _map_to_seq2seq(self):\n",
        "      self.dataset = self.dataset.map(\n",
        "          lambda example: {\n",
        "              'text': self.prefix + ' ' + self.prefix1 + ': ' + example['sentence1'] + ' ' + self.prefix2 + ': ' + example['sentence2'], \n",
        "              'labels':  str(round(example['similarity_score'], 1))\n",
        "          }, \n",
        "          remove_columns=['sentence1', 'sentence2', 'similarity_score']\n",
        "      )\n",
        "    \n",
        "    def get_data(self):\n",
        "      return self.dataset\n",
        "    \n",
        "    def map_string2id(self, string_label: str):\n",
        "        try:\n",
        "          return float(string_label)\n",
        "        except ValueError:\n",
        "          return -1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "      return load_metric('glue', 'stsb')\n",
        "\n",
        "\n",
        "class QNLI(Task):\n",
        "    prefix = 'qnli'\n",
        "    prefix1 = 'otázka'  # 'question' \n",
        "    prefix2 = 'veta'   # 'sentence'\n",
        "    id2string = {0: 'nevyplýva', 1: 'vyplýva'}  # {0: 'entailment', 1: 'not_entailment'}\n",
        "    string2id = {v: k for k, v in id2string.items()}\n",
        "    seed = 7\n",
        "\n",
        "    def __init__(self, seq2seq=True):\n",
        "        self._prepare_data()\n",
        "        if seq2seq:\n",
        "          self._map_to_seq2seq()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "      self.dataset = load_dataset('crabz/boolq_sk') \n",
        "      self.dataset = self.dataset.map(lambda example: {'answer': int(example['answer'])})  # True -> 1, False -> 0\n",
        "      dev_test = self.dataset['validation'].train_test_split(shuffle=True, seed=self.seed, test_size=0.7)\n",
        "      self.dataset['validation'] = dev_test['train']\n",
        "      self.dataset['test'] = dev_test['test']\n",
        "    \n",
        "    def _map_to_seq2seq(self):\n",
        "      self.dataset = self.dataset.map(\n",
        "          lambda example: {\n",
        "              'text': self.prefix + ' ' + self.prefix1 + ': ' +  example['question'] + ' ' + self.prefix2 + ': ' + example['passage'], \n",
        "              'labels': self.id2string[example['answer']]\n",
        "          }, \n",
        "          remove_columns=['question', 'passage', 'answer']\n",
        "      )\n",
        "    \n",
        "    def get_data(self):\n",
        "      return self.dataset\n",
        "      \n",
        "    def map_string2id(self, string_label: str):\n",
        "        try:\n",
        "          return self.string2id[string_label]\n",
        "        except KeyError:\n",
        "          return -1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "      return load_metric('glue', 'qnli')\n",
        "  \n",
        "\n",
        "def get_slovak_glue_task(task, seq2seq=True):\n",
        "      tasks = {\n",
        "            'sst2': SST2,\n",
        "            'stsb': STSB,\n",
        "            'qnli': QNLI\n",
        "      \n",
        "      }\n",
        "  \n",
        "      if task not in set(tasks.keys()):\n",
        "        raise KeyError(f'Not valid task, pleas choose from: {tasks.keys()}')\n",
        "      else:\n",
        "        return tasks[task](seq2seq)\n"
      ],
      "metadata": {
        "id": "bQdmsYxDeIw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args():\n",
        "    args = config_dict.ConfigDict()\n",
        "    args.model_name = 'ApoTro/slovak-t5-small'\n",
        "    args.tokenizer_name = 'ApoTro/slovak-t5-small'\n",
        "\n",
        "    args.task = 'sst2'\n",
        "    args.max_input_length = 512\n",
        "    args.max_target_length = 4\n",
        "\n",
        "    args.output_dir = './'\n",
        "    args.num_train_epochs = 1\n",
        "    args.learning_rate =   1e-4\n",
        "    args.per_device_train_batch_size = 12\n",
        "    args.per_device_eval_batch_size = 12\n",
        "    args.gradient_accumulation_steps = 2\n",
        "    args.eval_steps = 25\n",
        "    return args\n",
        "\n",
        "args = get_args()"
      ],
      "metadata": {
        "id": "dpTJ0KWeeMoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(args.model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)"
      ],
      "metadata": {
        "id": "PnIee6_Pegqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = get_slovak_glue_task(args.task)\n",
        "dataset = task.get_data()\n",
        "metric = task.get_metric()\n",
        "string2id = task.map_string2id"
      ],
      "metadata": {
        "id": "7mAKBX9aeZI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(lambda examples: tokenizer(examples['text'], max_length=args.max_input_length, truncation=True), remove_columns=['text', 'labels'])\n",
        "target_tokenized = dataset.map(lambda examples: tokenizer(examples['labels'], max_length=args.max_input_length, truncation=True, return_attention_mask=False), remove_columns=['text', 'labels'] )\n",
        "\n",
        "for s in ['train', 'test', 'validation']:\n",
        "  tokenized_datasets[s] = tokenized_datasets[s].add_column('labels', target_tokenized[s]['input_ids'])"
      ],
      "metadata": {
        "id": "f36TjMwZegTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_preds = [string2id(string_label) for string_label in decoded_preds]\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_labels = [string2id(string_label) for string_label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return result"
      ],
      "metadata": {
        "id": "JoJSNKLXe23P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "      output_dir=args.output_dir,\n",
        "      evaluation_strategy='steps',    \n",
        "      eval_steps=args.eval_steps,\n",
        "      num_train_epochs=args.num_train_epochs,\n",
        "      learning_rate=args.learning_rate,\n",
        "      per_device_train_batch_size=args.per_device_train_batch_size, \n",
        "      per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
        "      gradient_accumulation_steps=args.gradient_accumulation_steps,  \n",
        "      predict_with_generate=True\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "axmrH1xkfB4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Ef_s7sKBfTB9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}